Explication du code source
==========================

Cette section d√©crit en d√©tail les deux parties fondamentales du projet **Smart Waste Detection**,
qui repose sur l'utilisation de deux mod√®les YOLOv8 (You Only Look Once version 8) combin√©s dans une architecture innovante :

1. Le **mod√®le de d√©tection binaire** : d√©termine si un objet constitue un d√©chet ou non selon le contexte
2. Le **mod√®le de classification multi-classe** : identifie le type de d√©chet parmi 5 cat√©gories principales

Cette approche en pipeline permet d'optimiser les performances et la pr√©cision du syst√®me de reconnaissance.

------------------------------------------------------------
1. D√©tection binaire : Objet est-il un d√©chet ?
------------------------------------------------------------

Le premier mod√®le constitue la **porte d'entr√©e** du syst√®me. Il est entra√Æn√© pour **d√©tecter la pr√©sence d'un d√©chet dans une image** en tenant compte du contexte environnemental. Il ne classe pas le type de d√©chet, mais d√©termine si l'objet observ√© est consid√©r√© comme un **d√©chet** ou **non-d√©chet**.

**Code d'impl√©mentation :**

.. code-block:: python

   from ultralytics import YOLO
   import cv2
   import numpy as np

   # Chargement du mod√®le de d√©tection binaire
   model_detect = YOLO("/content/drive/MyDrive/yolov8_best_smartdetection.pt")

   # Pr√©diction sur une image
   results = model_detect("image.jpg")

   # Filtrage des objets identifi√©s comme d√©chets avec seuil de confiance
   confidence_threshold = 0.5
   waste_boxes = [
       box for box in results[0].boxes 
       if box.cls == 0 and box.conf >= confidence_threshold
   ]

   print(f"Nombre de d√©chets d√©tect√©s : {len(waste_boxes)}")

**Caract√©ristiques techniques :**

- **Architecture** : YOLOv8n (version nano pour rapidit√© d'ex√©cution)
- **Classes d'entra√Ænement** : 2 classes (`waste` et `non_waste`)
- **Fichier mod√®le** : `yolov8_best_smartdetection.pt`
- **D√©tection contextuelle** : Analyse l'environnement pour la classification
- **Seuil de confiance** : Filtrage des d√©tections peu fiables

**Avantages de cette approche :**

- **Pr√©-filtrage efficace** : R√©duit les faux positifs avant classification
- **Performance optimis√©e** : √âvite le traitement d'objets non pertinents
- **Contextualisation** : Prend en compte la situation de l'objet
- **Rapidit√© d'ex√©cution** : Mod√®le l√©ger pour traitement temps r√©el

------------------------------------------------------------
2. Classification multi-classe : Quel type de d√©chet ?
------------------------------------------------------------

Le deuxi√®me mod√®le est **sp√©cialis√©** pour classifier les objets qui ont √©t√© identifi√©s comme d√©chets lors de l'√©tape pr√©c√©dente. Cette segmentation permet d'obtenir une classification plus pr√©cise et cibl√©e.

**Code d'impl√©mentation :**

.. code-block:: python

   # Chargement du mod√®le de classification multi-classe
   model_classify = YOLO("/content/drive/MyDrive/yolov8_best.pt")

   def crop_image(image_path, bbox):
       """
       Fonction pour recadrer l'image selon les coordonn√©es de la bounding box
       
       Args:
           image_path (str): Chemin vers l'image source
           bbox (tensor): Coordonn√©es [x1, y1, x2, y2] de la bounding box
           
       Returns:
           numpy.ndarray: Image recadr√©e
       """
       image = cv2.imread(image_path)
       x1, y1, x2, y2 = map(int, bbox)
       cropped = image[y1:y2, x1:x2]
       return cropped

   # Traitement de chaque objet identifi√© comme d√©chet
   for i, box in enumerate(waste_boxes):
       # Extraction (recadrage) de la r√©gion d'int√©r√™t
       cropped_img = crop_image("image.jpg", box.xyxy[0])
       
       # Classification de l'objet recadr√©
       classification_result = model_classify(cropped_img)
       
       # Extraction des r√©sultats
       predicted_class = classification_result[0].probs.top1
       confidence = classification_result[0].probs.top1conf.item()
       class_name = classification_result[0].names[predicted_class]
       
       print(f"D√©chet {i+1}: {class_name} (confiance: {confidence:.2f})")

**Classes g√©r√©es par le mod√®le de classification :**

.. list-table::
   :header-rows: 1
   :widths: 15 25 60

   * - ID
     - Type de d√©chet
     - Description et exemples
   * - 0
     - Plastique
     - Bouteilles, emballages, sacs plastiques, contenants
   * - 1
     - Verre
     - Bouteilles en verre, pots, contenants transparents  
   * - 2
     - M√©tal
     - Canettes, emballages m√©talliques, capsules
   * - 3
     - Papier
     - Feuilles, journaux, documents, magazines
   * - 4
     - Carton
     - Bo√Ætes, emballages cartonn√©s, cartons ondul√©s

------------------------------------------------------------
3. Pipeline complet int√©gr√©
------------------------------------------------------------

Voici l'impl√©mentation compl√®te du pipeline combinant d√©tection binaire et classification multi-classe :

.. code-block:: python

   from ultralytics import YOLO
   import cv2
   import numpy as np
   from typing import List, Tuple, Dict

   class SmartWasteDetector:
       """
       Syst√®me de d√©tection et classification intelligente des d√©chets
       """
       
       def __init__(self, detection_model_path: str, classification_model_path: str):
           """
           Initialisation des mod√®les
           
           Args:
               detection_model_path (str): Chemin vers le mod√®le de d√©tection
               classification_model_path (str): Chemin vers le mod√®le de classification
           """
           self.model_detect = YOLO(detection_model_path)
           self.model_classify = YOLO(classification_model_path)
           
           # Classes de d√©chets
           self.waste_classes = {
               0: "Plastique",
               1: "Verre", 
               2: "M√©tal",
               3: "Papier",
               4: "Carton"
           }
       
       def crop_image(self, image: np.ndarray, bbox: List[float]) -> np.ndarray:
           """Recadrage de l'image selon la bounding box"""
           x1, y1, x2, y2 = map(int, bbox)
           return image[y1:y2, x1:x2]
       
       def process_image(self, image_path: str, confidence_threshold: float = 0.5) -> List[Dict]:
           """
           Traitement complet d'une image
           
           Args:
               image_path (str): Chemin vers l'image √† analyser
               confidence_threshold (float): Seuil de confiance minimum
               
           Returns:
               List[Dict]: Liste des d√©chets d√©tect√©s avec leurs caract√©ristiques
           """
           # Chargement de l'image
           image = cv2.imread(image_path)
           
           # √âtape 1: D√©tection binaire
           detection_results = self.model_detect(image)
           
           waste_detections = []
           
           # Filtrage des objets d√©tect√©s comme d√©chets
           for box in detection_results[0].boxes:
               if box.cls == 0 and box.conf >= confidence_threshold:
                   # √âtape 2: Recadrage de l'objet
                   cropped_img = self.crop_image(image, box.xyxy[0])
                   
                   # √âtape 3: Classification du type de d√©chet
                   classification_result = self.model_classify(cropped_img)
                   
                   # Extraction des informations
                   predicted_class = classification_result[0].probs.top1
                   class_confidence = classification_result[0].probs.top1conf.item()
                   
                   waste_info = {
                       'bbox': box.xyxy[0].tolist(),
                       'detection_confidence': box.conf.item(),
                       'waste_type': self.waste_classes[predicted_class],
                       'classification_confidence': class_confidence,
                       'coordinates': {
                           'x1': int(box.xyxy[0][0]),
                           'y1': int(box.xyxy[0][1]),
                           'x2': int(box.xyxy[0][2]),
                           'y2': int(box.xyxy[0][3])
                       }
                   }
                   
                   waste_detections.append(waste_info)
           
           return waste_detections

   # Utilisation du syst√®me complet
   detector = SmartWasteDetector(
       detection_model_path="/content/drive/MyDrive/yolov8_best_smartdetection.pt",
       classification_model_path="/content/drive/MyDrive/yolov8_best.pt"
   )

   # Analyse d'une image
   results = detector.process_image("test_image.jpg", confidence_threshold=0.6)

   # Affichage des r√©sultats
   for i, waste in enumerate(results):
       print(f"D√©chet {i+1}:")
       print(f"  Type: {waste['waste_type']}")
       print(f"  Confiance d√©tection: {waste['detection_confidence']:.2f}")
       print(f"  Confiance classification: {waste['classification_confidence']:.2f}")
       print(f"  Position: ({waste['coordinates']['x1']}, {waste['coordinates']['y1']}) -> ({waste['coordinates']['x2']}, {waste['coordinates']['y2']})")
       print("-" * 50)

------------------------------------------------------------
4. Optimisations et bonnes pratiques
------------------------------------------------------------

**Gestion des performances :**

.. code-block:: python

   # Configuration pour optimisation GPU
   import torch
   
   # V√©rification de la disponibilit√© CUDA
   device = 'cuda' if torch.cuda.is_available() else 'cpu'
   print(f"Dispositif utilis√©: {device}")
   
   # Optimisation m√©moire pour traitement par lots
   def process_batch(image_paths: List[str], batch_size: int = 4):
       """Traitement par lots pour optimiser les performances"""
       results = []
       for i in range(0, len(image_paths), batch_size):
           batch = image_paths[i:i + batch_size]
           batch_results = [detector.process_image(img) for img in batch]
           results.extend(batch_results)
       return results

**Gestion des erreurs :**

.. code-block:: python

   import logging

   def safe_process_image(self, image_path: str) -> List[Dict]:
       """Version s√©curis√©e du traitement d'image avec gestion d'erreurs"""
       try:
           if not os.path.exists(image_path):
               raise FileNotFoundError(f"Image non trouv√©e: {image_path}")
           
           results = self.process_image(image_path)
           logging.info(f"Traitement r√©ussi: {len(results)} d√©chets d√©tect√©s")
           return results
           
       except Exception as e:
           logging.error(f"Erreur lors du traitement de {image_path}: {str(e)}")
           return []

------------------------------------------------------------
5. D√©ploiement et int√©gration
------------------------------------------------------------

**Interface Streamlit :**

.. code-block:: python

   import streamlit as st
   
   st.title("üóëÔ∏è Smart Waste Detection")
   
   uploaded_file = st.file_uploader("Choisir une image", type=['jpg', 'jpeg', 'png'])
   
   if uploaded_file is not None:
       # Traitement de l'image
       results = detector.process_image(uploaded_file)
       
       # Affichage des r√©sultats
       if results:
           st.success(f"{len(results)} d√©chet(s) d√©tect√©(s)")
           for waste in results:
               st.write(f"**{waste['waste_type']}** - Confiance: {waste['classification_confidence']:.2f}")
       else:
           st.info("Aucun d√©chet d√©tect√© dans l'image")

**API REST avec FastAPI :**

.. code-block:: python

   from fastapi import FastAPI, File, UploadFile
   from fastapi.responses import JSONResponse
   
   app = FastAPI(title="Smart Waste Detection API")
   detector = SmartWasteDetector("model1.pt", "model2.pt")
   
   @app.post("/detect-waste/")
   async def detect_waste(file: UploadFile = File(...)):
       """Endpoint pour la d√©tection de d√©chets"""
       try:
           # Sauvegarde temporaire du fichier
           temp_path = f"temp_{file.filename}"
           with open(temp_path, "wb") as buffer:
               buffer.write(await file.read())
           
           # Traitement
           results = detector.process_image(temp_path)
           
           # Nettoyage
           os.remove(temp_path)
           
           return JSONResponse({
               "status": "success",
               "detections": len(results),
               "results": results
           })
           
       except Exception as e:
           return JSONResponse({
               "status": "error", 
               "message": str(e)
           }, status_code=500)

------------------------------------------------------------
6. M√©triques et √©valuation
------------------------------------------------------------

**Calcul des m√©triques de performance :**

.. code-block:: python

   def evaluate_model_performance(test_images: List[str], ground_truth: List[Dict]) -> Dict:
       """
       √âvaluation des performances du mod√®le
       
       Returns:
           Dict: M√©triques de performance (pr√©cision, rappel, F1-score)
       """
       true_positives = 0
       false_positives = 0
       false_negatives = 0
       
       for i, image_path in enumerate(test_images):
           predictions = detector.process_image(image_path)
           ground_truth_labels = ground_truth[i]
           
           # Calcul des m√©triques (simplifi√©)
           # Implementation d√©taill√©e selon vos crit√®res d'√©valuation
           
       precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
       recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
       f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
       
       return {
           'precision': precision,
           'recall': recall, 
           'f1_score': f1_score
       }

------------------------------------------------------------
7. Remarques techniques et optimisations
------------------------------------------------------------

**Consid√©rations importantes :**

- **Pr√©processing** : Normalisation des images pour coh√©rence des r√©sultats
- **Post-processing** : Filtrage des d√©tections selon seuils de confiance
- **Gestion m√©moire** : Lib√©ration des ressources apr√®s traitement
- **Batch processing** : Traitement par lots pour optimiser les performances
- **Cache des mod√®les** : √âviter le rechargement r√©p√©titif des mod√®les

**Optimisations avanc√©es :**

.. code-block:: python

   # Optimisation pour production
   class OptimizedWasteDetector(SmartWasteDetector):
       def __init__(self, *args, **kwargs):
           super().__init__(*args, **kwargs)
           # Pr√©chargement et optimisation des mod√®les
           self.model_detect.export(format='onnx')  # Export ONNX pour rapidit√©
           
       def preprocess_image(self, image: np.ndarray) -> np.ndarray:
           """Pr√©processing standardis√© des images"""
           # Redimensionnement, normalisation, etc.
           return cv2.resize(image, (640, 640))

------------------------------------------------------------
8. Tests et validation
------------------------------------------------------------

**Suite de tests unitaires :**

.. code-block:: python

   import unittest
   
   class TestSmartWasteDetector(unittest.TestCase):
       
       def setUp(self):
           self.detector = SmartWasteDetector("model1.pt", "model2.pt")
       
       def test_image_processing(self):
           """Test du traitement d'image basique"""
           results = self.detector.process_image("test_image.jpg")
           self.assertIsInstance(results, list)
       
       def test_crop_functionality(self):
           """Test de la fonction de recadrage"""
           image = np.zeros((100, 100, 3), dtype=np.uint8)
           cropped = self.detector.crop_image(image, [10, 10, 50, 50])
           self.assertEqual(cropped.shape, (40, 40, 3))

------------------------------------------------------------
9. Conclusion technique
------------------------------------------------------------

L'architecture propos√©e offre plusieurs **avantages significatifs** :

**Performance et pr√©cision :**
- **R√©duction des faux positifs** gr√¢ce au pr√©-filtrage binaire
- **Classification sp√©cialis√©e** pour une meilleure pr√©cision typologique  
- **Traitement optimis√©** avec pipeline s√©quentiel efficace

**Flexibilit√© et √©volutivit√© :**
- **Mod√®les ind√©pendants** permettant l'am√©lioration s√©par√©e
- **Architecture modulaire** facilitant l'int√©gration
- **D√©ploiement adaptatif** (local, cloud, edge computing)

**Applications pratiques :**
- **Temps r√©el** : Cam√©ras de surveillance environnementale
- **Traitement par lots** : Analyse de grandes quantit√©s d'images
- **Interface utilisateur** : Applications web et mobile
- **API REST** : Int√©gration dans syst√®mes existants

Cette approche **dual-model** constitue une solution robuste et scalable pour la d√©tection intelligente des d√©chets, ouvrant la voie vers des applications industrielles et environnementales concr√®tes.

üìû Contact & Support
----------------------

.. raw:: html

   <div style="background-color: #28a745; padding: 20px; border-radius: 10px; margin: 20px 0; box-shadow: 0 4px 8px rgba(0,0,0,0.1); text-align: center;">
      <div style="color: white; font-family: 'Arial', sans-serif;">
         <h3 style="margin: 0 0 15px 0; font-size: 1.4em; font-weight: bold;">
            D√©velopp√© par Youssef ES-SAAIDI & Zakariae ZEMMAHI & Mohamed HAJJI
         </h3>
         <div style="display: flex; justify-content: center; gap: 30px; flex-wrap: wrap; margin-top: 15px;">
            <div style="display: flex; align-items: center; gap: 8px;">
               <span style="font-size: 1.2em;">üêô</span>
               <a href="https://github.com/YoussefAIDT" target="_blank" style="color: #ffffff; text-decoration: none; font-weight: 500; padding: 5px 10px; background-color: rgba(255,255,255,0.2); border-radius: 5px; transition: all 0.3s ease;">
                  YoussefAIDT GitHub
               </a>
            </div>
            <div style="display: flex; align-items: center; gap: 8px;">
               <span style="font-size: 1.2em;">üêô</span>
               <a href="https://github.com/zakariazemmahi" target="_blank" style="color: #ffffff; text-decoration: none; font-weight: 500; padding: 5px 10px; background-color: rgba(255,255,255,0.2); border-radius: 5px; transition: all 0.3s ease;">
                  zakariazemmahi GitHub
               </a>
            </div>
            <div style="display: flex; align-items: center; gap: 8px;">
               <span style="font-size: 1.2em;">üêô</span>
               <a href="https://github.com/mohamedhajji11" target="_blank" style="color: #ffffff; text-decoration: none; font-weight: 500; padding: 5px 10px; background-color: rgba(255,255,255,0.2); border-radius: 5px; transition: all 0.3s ease;">
                  mohamedhajji11 GitHub
               </a>
            </div>
         </div>
      </div>
   </div>

.. raw:: html

   <style>
   div a:hover {
      background-color: rgba(255,255,255,0.3) !important;
      transform: translateY(-2px);
   }
   </style>
